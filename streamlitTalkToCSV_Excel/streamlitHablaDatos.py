import streamlit as st
import pandas as pd
from langchain_experimental.agents import create_pandas_dataframe_agent
from langchain_groq import ChatGroq


def reiniciarChat():
    """Funci칩n que reinicia el chat cuando se cambia de archivo
    """    
    st.toast("Archivo cargado",icon='游늯')
    # Inicializamos el historial de chat
    if "messages" in st.session_state:
        st.session_state.messages = []
        promtpSistema = "Vas a actuar como un analista de datos experto, dando siempre respuestas claras y concretas y siempre en idioma espa침ol, si te piden tablas o listas, las generas siempre en markdown"
        st.session_state.messages.append({"role": "system", "content": promtpSistema})

llm = ChatGroq(
    model="llama3-70b-8192",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key=st.secrets["GROQ_API"],
    
)

# Definimos los par치metros de configuraci칩n de la aplicaci칩n
st.set_page_config(
    page_title="Habla con tus datos", #T칤tulo de la p치gina
    page_icon="游늵", # 칈cono
    layout="wide", # Forma de layout ancho o compacto
    initial_sidebar_state="expanded" # Definimos si el sidebar aparece expandido o colapsado
)

st.header('Habla con tus datos')

# Men칰 lateral
with st.sidebar:
    st.subheader('Par치metros')
    archivo_cargado = st.file_uploader("Elige un archivo",type=['csv','xls','xlsx'],on_change =reiniciarChat)
    parUsarMemoria = st.checkbox("Recordar la conversacion",value=True)
    # Si existe un archivo cargado ejecutamos el c칩digo
    if archivo_cargado is not None:           
        # Se puede cargar con pandas si se tiene detectado el tipo de archivo
        if '.csv' in archivo_cargado.name:
            df = pd.read_csv(archivo_cargado)
        elif '.xls' in archivo_cargado.name:
            df = pd.read_excel(archivo_cargado)
        # Creamos el agente con el dataframe
        agent = create_pandas_dataframe_agent(llm,df,allow_dangerous_code=True)    
# Inicializamos el historial de chat
if "messages" not in st.session_state:
    st.session_state.messages = []
    
   

# Muestra mensajes de chat desde la historia en la aplicaci칩n cada vez que la aplicaci칩n se ejecuta
with st.container():
    for message in st.session_state.messages:
        if message["role"]!="system": # Omitimos el prompt de sistem
            with st.chat_message(message["role"]):
                st.markdown(message["content"])


# Mostramos el campo para el prompt del usuario
prompt=st.chat_input("Qu칠 quieres saber?")


if prompt:
     # Mostrar mensaje de usuario en el contenedor de mensajes de chat
    st.chat_message("user").markdown(prompt)
    # Agregar mensaje de usuario al historial de chat
    st.session_state.messages.append({"role": "user", "content": prompt})
    # Si requerimos usar la memoria entregamos siempre el historial de chat
    if parUsarMemoria:
        messages=[                
                    {
                        "role": m["role"],
                        "content": m["content"]
                    }
                    for m in st.session_state.messages
                ]
    else:
        # Si no se usa la memoria solo se entrega el prompt de sistema y la consulta del usuario
        messages=[                
                    {
                        "role": m["role"],
                        "content": m["content"]
                    }
                    for m in [st.session_state.messages[0],st.session_state.messages[-1]]
                ]    
    respuesta=agent.run(messages)
    
    # Mostrar respuesta del asistente en el contenedor de mensajes de chat
    with st.chat_message("assistant"):                        
        # Mostramos la respuesta
        st.write(respuesta)                                    
    # Agregar respuesta de asistente al historial de chat
    st.session_state.messages.append({"role": "assistant", "content": respuesta})
    